{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa15e33d",
   "metadata": {},
   "source": [
    "## Description of Notebook\n",
    "\n",
    "The notebook is used to look closer into VDFs and the fields files in order to identify the stable and unstable periods. This notebook is also used to prepare the data for the machine learning (yet without the partitioning of the data into the train and test data sets which will be done separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6223e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import kineticsim_reader as kr\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "from scipy.signal import savgol_filter\n",
    "from tqdm import tqdm\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb824ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "simfiles = ['particles.d11_A0.5Hepp_beta0.5eps1e-4_256',\\\n",
    "    'particles.d11_A0.75Hepp_beta1_256',\\\n",
    "    'particles.d11_E11Ap3.3Aa2.0Vd0.42',\\\n",
    "    'particles.d11_E11Ap4.3Aa1.6',\\\n",
    "    'particles.d11_E11Ap4.3Aa1.6Vd0.32',\\\n",
    "    'particles.d11_E12Ap1.86Aa1.0Vd0.32_256_256x256',\\\n",
    "    'particles.d11_E12Ap1.86Aa1.0Vd0.32_512_256x256',\\\n",
    "    'particles.d11_He++A10_256_iden0eps0',\\\n",
    "    'particles.d11_He++v2_256_iden0eps1e-4t600',\\\n",
    "    'particles.d11_He++vd1.5_256_iden0eps1e-4',\\\n",
    "    'particles.d11_pv1.5_128_64_iden0eps1e-4_dx0.75_long',\\\n",
    "    'particles.d11_pv1Ap2Apb2betac0.214betab0.858_128_128x2_dx0.75_t3000',\\\n",
    "    'particles.d11_pv2a_128x3_iden0eps1e-4_dx0.75',\\\n",
    "    'particles.d11_pv2Ap1Ab1betac0.429betab0.858_128_128x2_dx0.75_t3000',\\\n",
    "    'particles.d11_pv2Ap1Ab2betac0.429betab0.858_128_128x2_dx0.75_t3000',\\\n",
    "    'particles.d11_pv2Ap2Apb2betac0.214betab0.858_128_128x2_dx0.75_t3000',\\\n",
    "    'particles.d11_pv2av2.3_128x3_iden0eps1e-4_dx0.75',\\\n",
    "    'particles.d11_pv2av2Ap1Aa1beta0.429_128_128x2_dx0.75_t3000',\\\n",
    "    'particles.d11_pv2av2_rdna0.03375_128x3_iden0eps1e-4_dx0.75_t6000',\\\n",
    "    'particles.d11_vap1.2Ap1Aa0.75_rdna_0.05',\\\n",
    "    'particles.d11_vap1.2Ap3.35Aa2.05rdna_0.007',\\\n",
    "    'particles.d11_vap1.5Ap1.5Aa1rdna_0.007']\n",
    "\n",
    "fldfiles = ['fields.d10_A0.5Hepp_beta0.5eps1e-4_256',\\\n",
    "    'fields.d10_A0.75Hepp_beta1_256',\\\n",
    "    'fields.d10_E11Ap3.3Aa2.0Vd0.42',\\\n",
    "    'fields.d10_E11Ap4.3Aa1.6',\\\n",
    "    'fields.d10_E11Ap4.3Aa1.6Vd0.32',\\\n",
    "    'fields.d10_E12Ap1.86Aa1.0Vd0.32_256_256x256',\\\n",
    "    'fields.d10_E12Ap1.86Aa1.0Vd0.32_512_256x256',\\\n",
    "    'fields.d10_He++A10_256_iden0eps0',\\\n",
    "    'fields.d10_He++v2_256_iden0eps1e-4t600',\\\n",
    "    'fields.d10_He++vd1.5_256_iden0eps1e-4',\\\n",
    "    'fields.d10_pv1.5_128_64_iden0eps1e-4_dx0.75_long',\\\n",
    "    'fields.d10_pv1Ap2Apb2betac0.214betab0.858_128_128x2_dx0.75_t3000',\\\n",
    "    'fields.d10_pv2a_128x3_iden0eps1e-4_dx0.75',\\\n",
    "    'fields.d10_pv2Ap1Ab1betac0.429betab0.858_128_128x2_dx0.75_t3000',\\\n",
    "    'fields.d10_pv2Ap1Ab2betac0.429betab0.858_128_128x2_dx0.75_t3000',\\\n",
    "    'fields.d10_pv2Ap2Apb2betac0.214betab0.858_128_128x2_dx0.75_t3000',\\\n",
    "    'fields.d10_pv2av2.3_128x3_iden0eps1e-4_dx0.75',\\\n",
    "    'fields.d10_pv2av2Ap1Aa1beta0.429_128_128x2_dx0.75_t3000',\\\n",
    "    'fields.d10_pv2av2_rdna0.03375_128x3_iden0eps1e-4_dx0.75_t6000',\\\n",
    "    'fields.d10_vap1.2Ap1Aa0.75_rdna_0.05',\\\n",
    "    'fields.d10_vap1.2Ap3.35Aa2.05rdna_0.007',\\\n",
    "    'fields.d10_vap1.5Ap1.5Aa1rdna_0.007']\n",
    "\n",
    "populations = [[0.9,0.05], [0.9,0.05], [0.986,0.007], [0.986,0.007], [0.986,0.007],\\\n",
    "               [0.986,0.007], [0.986,0.007], [0.9,0.05], [0.9,0.05], [0.9,0.05], [1.00,0.00],\\\n",
    "               [1.00,0.00], [0.91,0.045], [1.00,0.00], [1.00,0.00], [1.00,0.00], [0.91,0.045],\\\n",
    "               [0.91,0.045], [0.91,0.045], [0.9,0.05], [0.986,0.007], [0.986,0.007]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16949b7c",
   "metadata": {},
   "source": [
    "## Determination of stable and unstable VDFs\n",
    "\n",
    "This overall remains an open question. Inspections of the VDFs and time parameters revealed the cases when the magnetic energy remains stable, but the anisotropies and temperatures change/exchange.\n",
    "\n",
    "Given the definition uncertainties above, we will implement several labeling approaches. First, the separate labelings will be developed for the magnetic energy and anisotropy following these thresholds:\n",
    "1. The simulation runs are classified: if the change of anisotropies or perpendicular magnetic energy is more than 0.1% per one gyroperiod, the VDFs are called unstable.\n",
    "2. The simulation runs are classified: if the change of anisotropies or perpendicular magnetic energy is more than 0.5% per one gyroperiod, the VDFs are called unstable.\n",
    "3. The simulation runs are classified: if the change of anisotropies or perpendicular magnetic energy is more than 1.0% per one gyroperiod, the VDFs are called unstable.\n",
    "4. The simulation runs are NOT classified. Instead, the regression problem will be solved for both the anisotropies and magnetic energies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb6001",
   "metadata": {},
   "source": [
    "The figures seem to be reasonable, except for the following cases where the labeling has to be adjusted:\n",
    "- Simulation run 'particles.d11_A0.5Hepp_beta0.5eps1e-4_256': overall, the entire run looks stable and should be counted as stable when solving for classification with the smallest threshold.\n",
    "- Simulation run 'particles.d11_A0.75Hepp_beta1_256': overall, the entire run looks stable and should be counted as stable when solving for classification with the smallest threshold.\n",
    "- All simulation runs: the very first and the very last points will be excluded to avoid the boundary effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd3b4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML data for the simulation particles.d11_A0.5Hepp_beta0.5eps1e-4_256 generated\n",
      "Number of data points: 78\n",
      "Positive samples with 0.1% (magnetic energy): 0\n",
      "ML data for the simulation particles.d11_A0.75Hepp_beta1_256 generated\n",
      "Number of data points: 46\n",
      "Positive samples with 0.1% (magnetic energy): 0\n",
      "ML data for the simulation particles.d11_E11Ap3.3Aa2.0Vd0.42 generated\n",
      "Number of data points: 46\n",
      "Positive samples with 0.1% (magnetic energy): 16\n",
      "ML data for the simulation particles.d11_E11Ap4.3Aa1.6 generated\n",
      "Number of data points: 46\n",
      "Positive samples with 0.1% (magnetic energy): 39\n",
      "ML data for the simulation particles.d11_E11Ap4.3Aa1.6Vd0.32 generated\n",
      "Number of data points: 46\n",
      "Positive samples with 0.1% (magnetic energy): 40\n",
      "ML data for the simulation particles.d11_E12Ap1.86Aa1.0Vd0.32_256_256x256 generated\n",
      "Number of data points: 48\n",
      "Positive samples with 0.1% (magnetic energy): 0\n",
      "ML data for the simulation particles.d11_E12Ap1.86Aa1.0Vd0.32_512_256x256 generated\n",
      "Number of data points: 48\n",
      "Positive samples with 0.1% (magnetic energy): 0\n",
      "ML data for the simulation particles.d11_He++A10_256_iden0eps0 generated\n",
      "Number of data points: 46\n",
      "Positive samples with 0.1% (magnetic energy): 45\n",
      "ML data for the simulation particles.d11_He++v2_256_iden0eps1e-4t600 generated\n",
      "Number of data points: 90\n",
      "Positive samples with 0.1% (magnetic energy): 74\n",
      "ML data for the simulation particles.d11_He++vd1.5_256_iden0eps1e-4 generated\n",
      "Number of data points: 78\n",
      "Positive samples with 0.1% (magnetic energy): 46\n",
      "ML data for the simulation particles.d11_pv1.5_128_64_iden0eps1e-4_dx0.75_long generated\n",
      "Number of data points: 148\n",
      "Positive samples with 0.1% (magnetic energy): 29\n",
      "ML data for the simulation particles.d11_pv1Ap2Apb2betac0.214betab0.858_128_128x2_dx0.75_t3000 generated\n",
      "Number of data points: 58\n",
      "Positive samples with 0.1% (magnetic energy): 1\n",
      "ML data for the simulation particles.d11_pv2a_128x3_iden0eps1e-4_dx0.75 generated\n",
      "Number of data points: 73\n",
      "Positive samples with 0.1% (magnetic energy): 9\n",
      "ML data for the simulation particles.d11_pv2Ap1Ab1betac0.429betab0.858_128_128x2_dx0.75_t3000 generated\n",
      "Number of data points: 58\n",
      "Positive samples with 0.1% (magnetic energy): 16\n",
      "ML data for the simulation particles.d11_pv2Ap1Ab2betac0.429betab0.858_128_128x2_dx0.75_t3000 generated\n",
      "Number of data points: 58\n",
      "Positive samples with 0.1% (magnetic energy): 1\n",
      "ML data for the simulation particles.d11_pv2Ap2Apb2betac0.214betab0.858_128_128x2_dx0.75_t3000 generated\n",
      "Number of data points: 58\n",
      "Positive samples with 0.1% (magnetic energy): 1\n",
      "ML data for the simulation particles.d11_pv2av2.3_128x3_iden0eps1e-4_dx0.75 generated\n",
      "Number of data points: 61\n",
      "Positive samples with 0.1% (magnetic energy): 24\n",
      "ML data for the simulation particles.d11_pv2av2Ap1Aa1beta0.429_128_128x2_dx0.75_t3000 generated\n",
      "Number of data points: 58\n",
      "Positive samples with 0.1% (magnetic energy): 16\n",
      "ML data for the simulation particles.d11_pv2av2_rdna0.03375_128x3_iden0eps1e-4_dx0.75_t6000 generated\n",
      "Number of data points: 35\n",
      "Positive samples with 0.1% (magnetic energy): 5\n",
      "ML data for the simulation particles.d11_vap1.2Ap1Aa0.75_rdna_0.05 generated\n",
      "Number of data points: 22\n",
      "Positive samples with 0.1% (magnetic energy): 0\n",
      "ML data for the simulation particles.d11_vap1.2Ap3.35Aa2.05rdna_0.007 generated\n",
      "Number of data points: 22\n",
      "Positive samples with 0.1% (magnetic energy): 1\n",
      "ML data for the simulation particles.d11_vap1.5Ap1.5Aa1rdna_0.007 generated\n",
      "Number of data points: 30\n",
      "Positive samples with 0.1% (magnetic energy): 0\n"
     ]
    }
   ],
   "source": [
    "def sort_labels_classification(rate_of_change):\n",
    "    label_001 = 0\n",
    "    label_005 = 0\n",
    "    label_01 = 0\n",
    "    label_05 = 0\n",
    "    label_10 = 0\n",
    "    if (rate_of_change > 0.0001): label_001 = 1\n",
    "    if (rate_of_change > 0.0005): label_005 = 1\n",
    "    if (rate_of_change > 0.001): label_01 = 1\n",
    "    if (rate_of_change > 0.005): label_05 = 1\n",
    "    if (rate_of_change > 0.010): label_10 = 1\n",
    "    return [label_001, label_005, label_01, label_05, label_10]\n",
    "\n",
    "def prepare_mldata_vdfmoments(simfile, fieldsfile, population):\n",
    "    \n",
    "    # loading arrays\n",
    "    timep_array = np.load('./processing_results/' + simfile + '.timep_array.npy')\n",
    "    anisotropies_p = np.load('./processing_results/' + simfile + '.anisotropies_p.npy')\n",
    "    moments_p = np.load('./processing_results/' + simfile + '.moments_p.npy')\n",
    "    anisotropies_he = np.load('./processing_results/' + simfile + '.anisotropies_he.npy')\n",
    "    moments_he = np.load('./processing_results/' + simfile + '.moments_he.npy')\n",
    "    \n",
    "    # loading VDFs and accounting for the case with large file VDF\n",
    "    if (simfile == 'particles.d11_pv1.5_128_64_iden0eps1e-4_dx0.75_long'):\n",
    "        vdfp_array_p1 = np.load('./processing_results/' + simfile + '_p1.vdfp_array.npy')\n",
    "        vdfhe_array_p1 = np.load('./processing_results/' + simfile + '_p1.vdfhe_array.npy')\n",
    "        vdfp_array_p2 = np.load('./processing_results/' + simfile + '_p2.vdfp_array.npy')\n",
    "        vdfhe_array_p2 = np.load('./processing_results/' + simfile + '_p2.vdfhe_array.npy')\n",
    "        vdfp_array = np.concatenate((vdfp_array_p1, vdfp_array_p2))\n",
    "        vdfhe_array = np.concatenate((vdfhe_array_p1, vdfhe_array_p2))\n",
    "    else:\n",
    "        vdfp_array = np.load('./processing_results/' + simfile + '.vdfp_array.npy')\n",
    "        vdfhe_array = np.load('./processing_results/' + simfile + '.vdfhe_array.npy')\n",
    "    \n",
    "    timep_array_fields = np.load('./processing_results/' + fieldsfile + '.timing.npy')[5:,1]\n",
    "    me_perp = np.load('./processing_results/' + fieldsfile + '.me_perp.npy')[5:]\n",
    "    me_tot = np.load('./processing_results/' + fieldsfile + '.me_tot.npy')[5:]\n",
    "    # applying smoothing to remove a periodic signal\n",
    "    dtime = timep_array_fields[1] - timep_array_fields[0]\n",
    "    npoints = int(len(me_tot)/10)\n",
    "    if (npoints % 2 == 0): npoints = npoints + 1\n",
    "    me_tot = savgol_filter(me_tot - me_tot[0], npoints, 3)\n",
    "    me_perp = savgol_filter(me_perp, npoints, 3)\n",
    "    \n",
    "    # resampling to the timing of the VDFs\n",
    "    me_tot = np.interp(timep_array,timep_array_fields,me_tot)\n",
    "    me_perp = np.interp(timep_array,timep_array_fields,me_perp)\n",
    "    \n",
    "    # time derivatives (relative)\n",
    "    dt_anisotropies_p = (anisotropies_p[1:]-anisotropies_p[:-1])/(timep_array[1:]-timep_array[:-1])\n",
    "    dt_anisotropies_p = 2*(dt_anisotropies_p)/(anisotropies_p[1:]+anisotropies_p[:-1])\n",
    "    if (anisotropies_he[0] == 0.0):\n",
    "        dt_anisotropies_he = anisotropies_he*0.0\n",
    "    else:\n",
    "        dt_anisotropies_he = (anisotropies_he[1:]-anisotropies_he[:-1])/(timep_array[1:]-timep_array[:-1])\n",
    "        dt_anisotropies_he = 2*(dt_anisotropies_he)/(anisotropies_he[1:]+anisotropies_he[:-1])\n",
    "    dt_me_perp = (me_perp[1:]-me_perp[:-1])/(timep_array[1:]-timep_array[:-1])\n",
    "    dt_me_perp = 2*(dt_me_perp)/(me_perp[1:]+me_perp[:-1])\n",
    "    dt_me_tot = (me_tot[1:]-me_tot[:-1])/(timep_array[1:]-timep_array[:-1])\n",
    "    dt_me_tot = 2*(dt_me_tot)/(me_tot[1:]+me_tot[:-1])\n",
    "    \n",
    "    # declaring feature vectors and moments\n",
    "    simnames = []\n",
    "    featurevector_allmoments = []\n",
    "    labels_allmoments_me_001 = []\n",
    "    labels_allmoments_me_005 = []\n",
    "    labels_allmoments_me_01 = []\n",
    "    labels_allmoments_me_05 = []\n",
    "    labels_allmoments_me_10 = []\n",
    "    labels_allmoments_an_001 = []\n",
    "    labels_allmoments_an_005 = []\n",
    "    labels_allmoments_an_01 = []\n",
    "    labels_allmoments_an_05 = []\n",
    "    labels_allmoments_an_10 = []\n",
    "    labels_allmoments_me_re = []\n",
    "    labels_allmoments_an_re = []\n",
    "    timep_array_out = []\n",
    "    # constructing the feature vector. The feature vector includes:\n",
    "    # - moments 0-3 along and across the field (taking care of b-parallel)\n",
    "    # - anisotropies in addition\n",
    "    # - particle relative populations in addition\n",
    "    # REMINDER: the very first and the very last time moments omitted\n",
    "    for i in range (1, len(timep_array)-2, 1):\n",
    "        subvector = []\n",
    "        subvector.append(moments_p[i,0,0])\n",
    "        subvector.append(moments_p[i,0,1])\n",
    "        subvector.append(moments_p[i,1,0])\n",
    "        subvector.append(moments_p[i,1,1])\n",
    "        subvector.append(moments_p[i,2,0])\n",
    "        subvector.append(moments_p[i,2,1])\n",
    "        subvector.append(moments_p[i,3,0])\n",
    "        subvector.append(moments_p[i,3,1])\n",
    "        subvector.append(moments_he[i,0,0])\n",
    "        subvector.append(moments_he[i,0,1])\n",
    "        subvector.append(moments_he[i,1,0])\n",
    "        subvector.append(moments_he[i,1,1])\n",
    "        subvector.append(moments_he[i,2,0])\n",
    "        subvector.append(moments_he[i,2,1])\n",
    "        subvector.append(moments_he[i,3,0])\n",
    "        subvector.append(moments_he[i,3,1])\n",
    "        subvector.append(anisotropies_p[i])\n",
    "        subvector.append(anisotropies_he[i])\n",
    "        subvector.append(population[0])\n",
    "        subvector.append(population[1])\n",
    "        # writing the common properties (rates and names) into the file\n",
    "        featurevector_allmoments.append(subvector)\n",
    "        simnames.append(simfile)\n",
    "        labels_allmoments_me_re.append(dt_me_perp[i])\n",
    "        labels_allmoments_an_re.append([dt_anisotropies_p[i], dt_anisotropies_he[i]])\n",
    "        timep_array_out.append(timep_array[i])\n",
    "        # sorting the labels depending on the strength of the change\n",
    "        labels_an_p = sort_labels_classification(np.abs(dt_anisotropies_p[i]))\n",
    "        labels_an_he = sort_labels_classification(np.abs(dt_anisotropies_he[i]))\n",
    "        labels_me = sort_labels_classification(np.abs(dt_me_perp[i]))\n",
    "        # fixing the cases for two specific simulations\n",
    "        if ((simfile == 'particles.d11_A0.5Hepp_beta0.5eps1e-4_256') or (simfile == 'particles.d11_A0.75Hepp_beta1_256')):\n",
    "            labels_me = [0,0,0,0,0]\n",
    "        # writing the labels into the arrays\n",
    "        labels_allmoments_me_001.append(labels_me[0])\n",
    "        labels_allmoments_me_005.append(labels_me[1])\n",
    "        labels_allmoments_me_01.append(labels_me[2])\n",
    "        labels_allmoments_me_05.append(labels_me[3])\n",
    "        labels_allmoments_me_10.append(labels_me[4])\n",
    "        # for anisotropies, one positive label is sufficient\n",
    "        labels_allmoments_an_001.append(np.amax([labels_an_p[0],labels_an_he[0]]))\n",
    "        labels_allmoments_an_005.append(np.amax([labels_an_p[1],labels_an_he[1]]))\n",
    "        labels_allmoments_an_01.append(np.amax([labels_an_p[2],labels_an_he[2]]))\n",
    "        labels_allmoments_an_05.append(np.amax([labels_an_p[3],labels_an_he[3]]))\n",
    "        labels_allmoments_an_10.append(np.amax([labels_an_p[4],labels_an_he[4]]))\n",
    "    \n",
    "    # converting to numpy arrays\n",
    "    simnames = np.array(simnames)\n",
    "    featurevector_allmoments = np.array(featurevector_allmoments, dtype=float)\n",
    "    labels_allmoments_me_001 = np.array(labels_allmoments_me_001, dtype=int)\n",
    "    labels_allmoments_me_005 = np.array(labels_allmoments_me_005, dtype=int)\n",
    "    labels_allmoments_me_01 = np.array(labels_allmoments_me_01, dtype=int)\n",
    "    labels_allmoments_me_05 = np.array(labels_allmoments_me_05, dtype=int)\n",
    "    labels_allmoments_me_10 = np.array(labels_allmoments_me_10, dtype=int)\n",
    "    labels_allmoments_an_001 = np.array(labels_allmoments_an_001, dtype=int)\n",
    "    labels_allmoments_an_005 = np.array(labels_allmoments_an_005, dtype=int)\n",
    "    labels_allmoments_an_01 = np.array(labels_allmoments_an_01, dtype=int)\n",
    "    labels_allmoments_an_05 = np.array(labels_allmoments_an_05, dtype=int)\n",
    "    labels_allmoments_an_10 = np.array(labels_allmoments_an_10, dtype=int)\n",
    "    labels_allmoments_me_re = np.array(labels_allmoments_me_re, dtype=float)\n",
    "    labels_allmoments_an_re = np.array(labels_allmoments_an_re, dtype=float)\n",
    "    timep_array_out = np.array(timep_array_out, dtype=float)\n",
    "    \n",
    "    # returning all arrays\n",
    "    return simnames, featurevector_allmoments, labels_allmoments_me_001, labels_allmoments_me_005, \\\n",
    "           labels_allmoments_me_01, labels_allmoments_me_05, \\\n",
    "           labels_allmoments_me_10, labels_allmoments_an_001, labels_allmoments_an_005, \\\n",
    "           labels_allmoments_an_01, labels_allmoments_an_05, labels_allmoments_an_10, \\\n",
    "           labels_allmoments_me_re, labels_allmoments_an_re, timep_array_out\n",
    "\n",
    "simnames, featurevector_allmoments, labels_allmoments_me_001, labels_allmoments_me_005, \\\n",
    "           labels_allmoments_me_01, labels_allmoments_me_05, \\\n",
    "           labels_allmoments_me_10, labels_allmoments_an_001, labels_allmoments_an_005, \\\n",
    "           labels_allmoments_an_01, labels_allmoments_an_05, labels_allmoments_an_10, \\\n",
    "           labels_allmoments_me_re, labels_allmoments_an_re, timep_array = \\\n",
    "    prepare_mldata_vdfmoments(simfiles[0], fldfiles[0], populations[0])\n",
    "print(\"ML data for the simulation \" + simfiles[0] + \" generated\")\n",
    "print(\"Number of data points: \" + str(len(labels_allmoments_me_01)))\n",
    "print(\"Positive samples with 0.1% (magnetic energy): \" + str(np.sum(labels_allmoments_me_01)))\n",
    "simnames_all = np.copy(simnames)\n",
    "featurevector_allmoments_all = np.copy(featurevector_allmoments)\n",
    "labels_allmoments_me_001_all = np.copy(labels_allmoments_me_001)\n",
    "labels_allmoments_me_005_all = np.copy(labels_allmoments_me_005)\n",
    "labels_allmoments_me_01_all = np.copy(labels_allmoments_me_01)\n",
    "labels_allmoments_me_05_all = np.copy(labels_allmoments_me_05)\n",
    "labels_allmoments_me_10_all = np.copy(labels_allmoments_me_10)\n",
    "labels_allmoments_an_001_all = np.copy(labels_allmoments_an_001)\n",
    "labels_allmoments_an_005_all = np.copy(labels_allmoments_an_005)\n",
    "labels_allmoments_an_01_all = np.copy(labels_allmoments_an_01)\n",
    "labels_allmoments_an_05_all = np.copy(labels_allmoments_an_05)\n",
    "labels_allmoments_an_10_all = np.copy(labels_allmoments_an_10)\n",
    "labels_allmoments_me_re_all = np.copy(labels_allmoments_me_re)\n",
    "labels_allmoments_an_re_all = np.copy(labels_allmoments_an_re)\n",
    "timep_array_all = np.copy(timep_array)\n",
    "\n",
    "for i in range (1, len(simfiles), 1):\n",
    "    simnames, featurevector_allmoments, labels_allmoments_me_001, labels_allmoments_me_005, \\\n",
    "           labels_allmoments_me_01, labels_allmoments_me_05, \\\n",
    "           labels_allmoments_me_10, labels_allmoments_an_001, labels_allmoments_an_005, \\\n",
    "           labels_allmoments_an_01, labels_allmoments_an_05, labels_allmoments_an_10, \\\n",
    "           labels_allmoments_me_re, labels_allmoments_an_re, timep_array = \\\n",
    "        prepare_mldata_vdfmoments(simfiles[i], fldfiles[i], populations[i])\n",
    "    print(\"ML data for the simulation \" + simfiles[i] + \" generated\")\n",
    "    print(\"Number of data points: \" + str(len(labels_allmoments_me_01)))\n",
    "    print(\"Positive samples with 0.1% (magnetic energy): \" + str(np.sum(labels_allmoments_me_01)))\n",
    "    \n",
    "    simnames_all = np.concatenate((simnames_all, simnames))\n",
    "    featurevector_allmoments_all = np.concatenate((featurevector_allmoments_all, featurevector_allmoments))\n",
    "    labels_allmoments_me_001_all = np.concatenate((labels_allmoments_me_001_all, labels_allmoments_me_001))\n",
    "    labels_allmoments_me_005_all = np.concatenate((labels_allmoments_me_005_all, labels_allmoments_me_005))\n",
    "    labels_allmoments_me_01_all = np.concatenate((labels_allmoments_me_01_all, labels_allmoments_me_01))\n",
    "    labels_allmoments_me_05_all = np.concatenate((labels_allmoments_me_05_all, labels_allmoments_me_05))\n",
    "    labels_allmoments_me_10_all = np.concatenate((labels_allmoments_me_10_all, labels_allmoments_me_10))\n",
    "    labels_allmoments_an_001_all = np.concatenate((labels_allmoments_an_001_all, labels_allmoments_an_001))\n",
    "    labels_allmoments_an_005_all = np.concatenate((labels_allmoments_an_005_all, labels_allmoments_an_005))\n",
    "    labels_allmoments_an_01_all = np.concatenate((labels_allmoments_an_01_all, labels_allmoments_an_01))\n",
    "    labels_allmoments_an_05_all = np.concatenate((labels_allmoments_an_05_all, labels_allmoments_an_05))\n",
    "    labels_allmoments_an_10_all = np.concatenate((labels_allmoments_an_10_all, labels_allmoments_an_10))\n",
    "    labels_allmoments_me_re_all = np.concatenate((labels_allmoments_me_re_all, labels_allmoments_me_re))\n",
    "    labels_allmoments_an_re_all = np.concatenate((labels_allmoments_an_re_all, labels_allmoments_an_re))\n",
    "    timep_array_all = np.concatenate((timep_array_all, timep_array))\n",
    "    \n",
    "np.save('./mldata_vdfmoments/allsimulations.simnames_all.npy', simnames_all)\n",
    "np.save('./mldata_vdfmoments/allsimulations.featurevector_allmoments_all.npy', featurevector_allmoments_all)\n",
    "np.save('./mldata_vdfmoments/allsimulations.labels_allmoments_me_001_all.npy', labels_allmoments_me_001_all)\n",
    "np.save('./mldata_vdfmoments/allsimulations.labels_allmoments_me_005_all.npy', labels_allmoments_me_005_all)\n",
    "np.save('./mldata_vdfmoments/allsimulations.labels_allmoments_me_01_all.npy', labels_allmoments_me_01_all)\n",
    "np.save('./mldata_vdfmoments/allsimulations.labels_allmoments_me_05_all.npy', labels_allmoments_me_05_all)\n",
    "np.save('./mldata_vdfmoments/allsimulations.labels_allmoments_me_10_all.npy', labels_allmoments_me_10_all)\n",
    "np.save('./mldata_vdfmoments/allsimulations.labels_allmoments_an_001_all.npy', labels_allmoments_an_001_all)\n",
    "np.save('./mldata_vdfmoments/allsimulations.labels_allmoments_an_005_all.npy', labels_allmoments_an_005_all)\n",
    "np.save('./mldata_vdfmoments/allsimulations.labels_allmoments_an_01_all.npy', labels_allmoments_an_01_all)\n",
    "np.save('./mldata_vdfmoments/allsimulations.labels_allmoments_an_05_all.npy', labels_allmoments_an_05_all)\n",
    "np.save('./mldata_vdfmoments/allsimulations.labels_allmoments_an_10_all.npy', labels_allmoments_an_10_all)\n",
    "np.save('./mldata_vdfmoments/allsimulations.labels_allmoments_me_re_all.npy', labels_allmoments_me_re_all)\n",
    "np.save('./mldata_vdfmoments/allsimulations.labels_allmoments_an_re_all.npy', labels_allmoments_an_re_all)\n",
    "np.save('./mldata_vdfmoments/allsimulations.timep_array_all.npy', timep_array_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e01b3",
   "metadata": {},
   "source": [
    "## Understanding Statistics of Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8331d177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.0001:\n",
      "Positive (ME): 768\n",
      "Negative (ME): 485\n",
      "Positive (AN): 949\n",
      "Negative (AN): 304\n",
      "Positive (Intersection): 634\n",
      "Positive (Union): 1083\n",
      "--------------------------------\n",
      "Threshold 0.0005:\n",
      "Positive (ME): 486\n",
      "Negative (ME): 767\n",
      "Positive (AN): 439\n",
      "Negative (AN): 814\n",
      "Positive (Intersection): 331\n",
      "Positive (Union): 594\n",
      "--------------------------------\n",
      "Threshold 0.001:\n",
      "Positive (ME): 363\n",
      "Negative (ME): 890\n",
      "Positive (AN): 240\n",
      "Negative (AN): 1013\n",
      "Positive (Intersection): 205\n",
      "Positive (Union): 398\n",
      "--------------------------------\n",
      "Threshold 0.005:\n",
      "Positive (ME): 93\n",
      "Negative (ME): 1160\n",
      "Positive (AN): 51\n",
      "Negative (AN): 1202\n",
      "Positive (Intersection): 36\n",
      "Positive (Union): 108\n",
      "--------------------------------\n",
      "Threshold 0.01:\n",
      "Positive (ME): 53\n",
      "Negative (ME): 1200\n",
      "Positive (AN): 28\n",
      "Negative (AN): 1225\n",
      "Positive (Intersection): 20\n",
      "Positive (Union): 61\n"
     ]
    }
   ],
   "source": [
    "print(\"Threshold 0.0001:\")\n",
    "print(\"Positive (ME):\", np.sum(labels_allmoments_me_001_all))\n",
    "print(\"Negative (ME):\", len(labels_allmoments_me_001_all) - np.sum(labels_allmoments_me_001_all))\n",
    "print(\"Positive (AN):\", np.sum(labels_allmoments_an_001_all))\n",
    "print(\"Negative (AN):\", len(labels_allmoments_an_001_all) - np.sum(labels_allmoments_an_001_all))\n",
    "print(\"Positive (Intersection):\", np.sum(labels_allmoments_an_001_all*labels_allmoments_me_001_all))\n",
    "print(\"Positive (Union):\", np.sum(labels_allmoments_me_001_all) + np.sum(labels_allmoments_an_001_all) - \\\n",
    "                           np.sum(labels_allmoments_an_001_all*labels_allmoments_me_001_all))\n",
    "print(\"--------------------------------\")\n",
    "print(\"Threshold 0.0005:\")\n",
    "print(\"Positive (ME):\", np.sum(labels_allmoments_me_005_all))\n",
    "print(\"Negative (ME):\", len(labels_allmoments_me_005_all) - np.sum(labels_allmoments_me_005_all))\n",
    "print(\"Positive (AN):\", np.sum(labels_allmoments_an_005_all))\n",
    "print(\"Negative (AN):\", len(labels_allmoments_an_005_all) - np.sum(labels_allmoments_an_005_all))\n",
    "print(\"Positive (Intersection):\", np.sum(labels_allmoments_an_005_all*labels_allmoments_me_005_all))\n",
    "print(\"Positive (Union):\", np.sum(labels_allmoments_me_005_all) + np.sum(labels_allmoments_an_005_all) - \\\n",
    "                           np.sum(labels_allmoments_an_005_all*labels_allmoments_me_005_all))\n",
    "print(\"--------------------------------\")\n",
    "print(\"Threshold 0.001:\")\n",
    "print(\"Positive (ME):\", np.sum(labels_allmoments_me_01_all))\n",
    "print(\"Negative (ME):\", len(labels_allmoments_me_01_all) - np.sum(labels_allmoments_me_01_all))\n",
    "print(\"Positive (AN):\", np.sum(labels_allmoments_an_01_all))\n",
    "print(\"Negative (AN):\", len(labels_allmoments_an_01_all) - np.sum(labels_allmoments_an_01_all))\n",
    "print(\"Positive (Intersection):\", np.sum(labels_allmoments_an_01_all*labels_allmoments_me_01_all))\n",
    "print(\"Positive (Union):\", np.sum(labels_allmoments_me_01_all) + np.sum(labels_allmoments_an_01_all) - \\\n",
    "                           np.sum(labels_allmoments_an_01_all*labels_allmoments_me_01_all))\n",
    "print(\"--------------------------------\")\n",
    "print(\"Threshold 0.005:\")\n",
    "print(\"Positive (ME):\", np.sum(labels_allmoments_me_05_all))\n",
    "print(\"Negative (ME):\", len(labels_allmoments_me_05_all) - np.sum(labels_allmoments_me_05_all))\n",
    "print(\"Positive (AN):\", np.sum(labels_allmoments_an_05_all))\n",
    "print(\"Negative (AN):\", len(labels_allmoments_an_05_all) - np.sum(labels_allmoments_an_05_all))\n",
    "print(\"Positive (Intersection):\", np.sum(labels_allmoments_an_05_all*labels_allmoments_me_05_all))\n",
    "print(\"Positive (Union):\", np.sum(labels_allmoments_me_05_all) + np.sum(labels_allmoments_an_05_all) - \\\n",
    "                           np.sum(labels_allmoments_an_05_all*labels_allmoments_me_05_all))\n",
    "print(\"--------------------------------\")\n",
    "print(\"Threshold 0.01:\")\n",
    "print(\"Positive (ME):\", np.sum(labels_allmoments_me_10_all))\n",
    "print(\"Negative (ME):\", len(labels_allmoments_me_10_all) - np.sum(labels_allmoments_me_10_all))\n",
    "print(\"Positive (AN):\", np.sum(labels_allmoments_an_10_all))\n",
    "print(\"Negative (AN):\", len(labels_allmoments_an_10_all) - np.sum(labels_allmoments_an_10_all))\n",
    "print(\"Positive (Intersection):\", np.sum(labels_allmoments_an_10_all*labels_allmoments_me_10_all))\n",
    "print(\"Positive (Union):\", np.sum(labels_allmoments_me_10_all) + np.sum(labels_allmoments_an_10_all) - \\\n",
    "                           np.sum(labels_allmoments_an_10_all*labels_allmoments_me_10_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb327398",
   "metadata": {},
   "source": [
    "It seems that, overall for the classification problem, it is best to use the 0.001 threshold (which will make the data set more balanced in terms of positive/negative sampling). Higher thresholds are highly-imbalanced. In addition, it is possibly best to consider the situation when either anisotropy or magnetic energy changes as imbalanced, which will bring the total number of positive cases to 398 out of 1253 VDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc44bcfb",
   "metadata": {},
   "source": [
    "However, it also might be that the threshold of 0.0005 or even 0.0001 is good enough. At least, the intersections/unions for these thresholds look balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6357bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
